CEO is the operating system and the other LLM serves are called workers.

List of LLM's -> https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard

Make local AI agent -> LLama(Meta) - opensource
Make agents with model like GPT for Omni

LLM's have different models
- Llama-2,

### What is LLM
Its basically two files.
- File-1 - Parameter file (like a zip file)
    - The parameter file is similar to a zip file, so we simply compress ten terabyte text into a 140 gigabyte file, and this file has 70 billion parameter.
    - Eg) LLM->Llama-2, Model->70b, Means it has 70billion parameters.
    - To get all the parameter, we train this file.
        - We use 10 terabyte of text(all over the internet) to train this file.
    - parameter file is compressed to 140GB.
        - For compressing we need a lot of GPU power.
        - GPU required to compress 10TB of text down to a Parameter file.
- File-1 - Run the parameter - Run file, written in C/Python
    - 500 lines of code
    - We can download the open source LLM file like Llama-2 or Llama-3 and run this file locally.(data security, nothing goes over internet)
    - Downfall In using closed source LLM's uses either a "web interface" or their "API" the issues in the data going over internet

### Three phases of LLM training
- Pre training: We simply use a lot of GPU to compress a lot of text down into a smaller so-called zip file and we can hallucinate text out of these. In order to make these hallucinations better. We do the fine tuning,
- Fine tuning:  we feed a lot of questions with answers structured in a way that humans like. And in this phase, the lab learns how humans want their responses. This is a lot cheaper. We need less GPUs. We do this with roughly 100,000 examples.
- Reinforcement learning: We simply take a look. Hey, makes the sense or not? If yes, thumbs up if no thumbs down and LM will simply learn further how we want our responses.

### What after learning?
Now, you have already learned it. In this "transformer architecture, there are neural nets". The neural nets work with weights. Basically, they work with numbers.

And in order to make sense for the neural net, of course we need to have numbers. So the first thing is, of course, if we feed the question into an LLM, the LLM will make numbers out of these questions. The so-called tokens. These tokens are numbers. And with these numbers the neural net can make its calculations. What word will come most likely as the next word?

Every single LLM has always a limit how many tokens it can understand right now. GPT four turbo and also GPT for Omni and a lot of other models. They have roughly 128,000 tokens as token limit. We have also models that have two millions in token limit. We have also small or open source model that have only 4000 tokens token limit.

And the important stuff is that you understand that as soon as this token limit is reached, the LLM will no longer understand the things that you talked previously with the LLM.

So basically, if you ever wonder why the LM no longer knows what you talked about previously, it's simply because the token limit is reached.